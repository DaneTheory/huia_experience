{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT WORKING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install onnx\n",
    "#!pip install onnx_tf\n",
    "# pip install -e . https://github.com/onnx/onnx-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/common/__init__.py:87: UserWarning: onnx_tf.common.get_outputs_names is deprecated. It will be removed in future release. Use TensorflowGraph.get_outputs_names instead.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/common/__init__.py:87: UserWarning: FrontendHandler.get_outputs_names is deprecated. It will be removed in future release. Use node.outputs instead.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('1.13.1', '1.5.0', '1.16.3')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx_tf.backend import prepare # install from master\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "tf.__version__ , onnx.__version__, numpy.__version__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load(\"huia2.onnx\")  # load onnx model\n",
    "#output = prepare(onnx_model).run(input)  # run the loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_rep.export_graph('huia_tensor_gpu.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflowjs_converter \\\n",
    "    --input_format=tf_frozen_model \\\n",
    "    --output_node_names='MobilenetV1/Predictions/Reshape_1' \\\n",
    "    /huia_tensor.pb \\\n",
    "    /huia_tensor_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# convert array order from pytorch to tensorflow\n",
    "img_raw = tf.io.read_file('./training_data/images/backpack_107.png')\n",
    "with tf.Session().as_default():\n",
    "    image = tf.io.decode_image(img_raw,channels=3).eval()\n",
    "    resized_image = tf.image.resize_images(image, [224, 224]).eval()\n",
    "\n",
    "#normalize image\n",
    "#offset = 127.5;\n",
    "#resized_image = (resized_image-offset)/offset  # convert to -1 : 1 \n",
    "\n",
    "\n",
    "imagenet_stats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "mean = imagenet_stats[0]\n",
    "std = imagenet_stats[1]\n",
    "\n",
    "def normalize(x, mean,std):\n",
    "    for i in range(0,2):\n",
    "        x[i] = x[i]-mean[i]/std[i]    \n",
    "    return x\n",
    "\n",
    "print(resized_image/255)\n",
    "\n",
    "norm_image = normalize(resized_image/255,mean,std)\n",
    "\n",
    "\n",
    "# transpose image\n",
    "trans_image =  np.transpose(norm_image, (2, 0, 1))\n",
    "trans_image.shape\n",
    "\n",
    "# create extra dimension\n",
    "exp_img = np.expand_dims(trans_image, axis=0)\n",
    "exp_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_CLASSES =['backpack',\n",
    " 'dramatic',\n",
    " 'fly',\n",
    " 'hadouken',\n",
    " 'moonwalk',\n",
    " 'normal',\n",
    " 'underarm',\n",
    " 'wings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/common/handler_helper.py:73: UserWarning: Unknown op ConstantFill in domain `ai.onnx`.\n",
      "  handler.ONNX_OP, handler.DOMAIN or \"ai.onnx\"))\n",
      "/home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/common/handler_helper.py:73: UserWarning: Unknown op DynamicSlice in domain `ai.onnx`.\n",
      "  handler.ONNX_OP, handler.DOMAIN or \"ai.onnx\"))\n",
      "/home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/common/handler_helper.py:73: UserWarning: Unknown op ImageScaler in domain `ai.onnx`.\n",
      "  handler.ONNX_OP, handler.DOMAIN or \"ai.onnx\"))\n",
      "/home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/common/handler_helper.py:70: UserWarning: Fail to get since_version of ThresholdedRelu in domain `` with max_inclusive_version=9. Set to 1.\n",
      "  handler.ONNX_OP, handler.DOMAIN, version))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/handlers/backend/pool_mixin.py:187: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/handlers/backend/pool_mixin.py:124: UserWarning: Using the pooling op in compatibility mode.This means your graph cannot be serialized.Please configure your pooling operation to only use paddings that correspond to Tensorflow SAME or VALID padding.\n",
      "  \"correspond to Tensorflow SAME or VALID padding.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/handlers/backend/reshape.py:31: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
      "WARNING:tensorflow:From /home/ale/Dropbox/projects/onnx-tensorflow/onnx_tf/handlers/backend/gemm.py:14: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n"
     ]
    }
   ],
   "source": [
    "tf_rep = prepare(onnx_model,device='CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal\n"
     ]
    }
   ],
   "source": [
    "output = tf_rep.run(exp_img) #.eval()  # run the loaded model\n",
    "print(POSE_CLASSES[np.argmax(output)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Outputs(_0=array([[  1.2426745,  14.113052 , -23.66965  ,  44.07164  , -14.209565 ,\n",
       "         16.602457 , -29.920212 , -47.434143 ]], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensor20] *",
   "language": "python",
   "name": "conda-env-tensor20-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
